{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A B2B company conducted an **acquisition campaign** in which they tried to convert leads into customers.\n",
    "  In addition, they registered which leads eventually converted into profitable customers and which leads didn't.\n",
    "    \n",
    "    \n",
    "- The **B2B** company also gathered information about the **characteristics of the leads**:\n",
    "    \n",
    "   \n",
    "    a) The commercial dataset: this is a dataset which was bought from commercial vendors who are specialized in\n",
    "       collecting data from companies such as revenue, net profit, cashflow, number of employees etc. \n",
    "       This data is often expensive and contains a lot of missing values.\n",
    "    \n",
    "    \n",
    "    a) The web dataset: this is a dataset which was scraped from the web.\n",
    "       This is a very cheap source of information but needs a lot of preprocessing since it is unstructured.\n",
    "       Initially, it contained the textual data found on the website (if available) of each company. \n",
    "       In a next step, a text mining algorithm (Singular Value Decomposition) was used to convert this textual \n",
    "       data in 200 numerical variables (SVD_1, SVD_2, ..., SVD_200). \n",
    "       These variables somehow represent concepts which were found across all the\n",
    "       websites and the values for these variables represent how much a company focused on this concept on its \n",
    "       website. This dataset also contains the Target variable which represents whether a company was converted \n",
    "       into a profitable customer or not.\n",
    "      \n",
    "- The B2B company wants to **build a model** that tries to find a relationship between the characteristics of the leads on the one hand and the probability of converting into a profitable customer on the other hand. This would allow the B2B company to **only target leads with a high probability of converting** into a profitable customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pictures gives a **quick overview** of the case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Data/acquisition_case_workflow.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following picture visualizes how **Singular Value Decomposition** (**SVD**) works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/lsi.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import web data\n",
    "web_data = pd.read_csv(\"../case_acquisition/data/web1.csv\", encoding=\"latin1\")\n",
    "\n",
    "# import commercial data, but with NACE_code as a string variable to keep leading zero \n",
    "commercial_data = pd.read_csv(\"../case_acquisition/data/commercial1.csv\", encoding=\"latin1\", dtype={'NACE_code': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 URI  \\\n0  file://C:\\sas\\small2\\n2-1313http%3A--www.melis...   \n1  file://C:\\sas\\small2\\n2-1327http%3A--www.auto-...   \n2  file://C:\\sas\\small2\\n2-1346http%3A--www.ac-me...   \n\n                                             NAME  TARGET  _DOCUMENT_  \\\n0        n2-1313http%3A--www.melisard.de-.txt.htm       0           1   \n1    n2-1327http%3A--www.auto-boehler.de-.txt.htm       0           2   \n2  n2-1346http%3A--www.ac-metallteile.de-.txt.htm       0           3   \n\n     _SVD_1    _SVD_2    _SVD_3    _SVD_4    _SVD_5    _SVD_6  ...  _SVD_192  \\\n0  0.390678 -0.118327  0.199925 -0.200753  0.053916 -0.104150  ... -0.013452   \n1  0.463090 -0.121661 -0.149195  0.222979 -0.097737  0.000127  ... -0.066052   \n2  0.425443 -0.192835 -0.162892  0.221980  0.106563 -0.120675  ...  0.010257   \n\n   _SVD_193  _SVD_194  _SVD_195  _SVD_196  _SVD_197  _SVD_198  _SVD_199  \\\n0  0.011185 -0.002033  0.005218 -0.007452  0.007244 -0.005440  0.012882   \n1  0.028640  0.020099 -0.013749 -0.006084 -0.004447  0.032138  0.020648   \n2  0.006590  0.058038  0.016583 -0.005697 -0.013547 -0.005410 -0.015903   \n\n   _SVD_200   _SVDLEN_  \n0 -0.000899  62.478443  \n1 -0.009978  36.631883  \n2  0.006741  43.768915  \n\n[3 rows x 205 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>URI</th>\n      <th>NAME</th>\n      <th>TARGET</th>\n      <th>_DOCUMENT_</th>\n      <th>_SVD_1</th>\n      <th>_SVD_2</th>\n      <th>_SVD_3</th>\n      <th>_SVD_4</th>\n      <th>_SVD_5</th>\n      <th>_SVD_6</th>\n      <th>...</th>\n      <th>_SVD_192</th>\n      <th>_SVD_193</th>\n      <th>_SVD_194</th>\n      <th>_SVD_195</th>\n      <th>_SVD_196</th>\n      <th>_SVD_197</th>\n      <th>_SVD_198</th>\n      <th>_SVD_199</th>\n      <th>_SVD_200</th>\n      <th>_SVDLEN_</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>file://C:\\sas\\small2\\n2-1313http%3A--www.melis...</td>\n      <td>n2-1313http%3A--www.melisard.de-.txt.htm</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.390678</td>\n      <td>-0.118327</td>\n      <td>0.199925</td>\n      <td>-0.200753</td>\n      <td>0.053916</td>\n      <td>-0.104150</td>\n      <td>...</td>\n      <td>-0.013452</td>\n      <td>0.011185</td>\n      <td>-0.002033</td>\n      <td>0.005218</td>\n      <td>-0.007452</td>\n      <td>0.007244</td>\n      <td>-0.005440</td>\n      <td>0.012882</td>\n      <td>-0.000899</td>\n      <td>62.478443</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>file://C:\\sas\\small2\\n2-1327http%3A--www.auto-...</td>\n      <td>n2-1327http%3A--www.auto-boehler.de-.txt.htm</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.463090</td>\n      <td>-0.121661</td>\n      <td>-0.149195</td>\n      <td>0.222979</td>\n      <td>-0.097737</td>\n      <td>0.000127</td>\n      <td>...</td>\n      <td>-0.066052</td>\n      <td>0.028640</td>\n      <td>0.020099</td>\n      <td>-0.013749</td>\n      <td>-0.006084</td>\n      <td>-0.004447</td>\n      <td>0.032138</td>\n      <td>0.020648</td>\n      <td>-0.009978</td>\n      <td>36.631883</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>file://C:\\sas\\small2\\n2-1346http%3A--www.ac-me...</td>\n      <td>n2-1346http%3A--www.ac-metallteile.de-.txt.htm</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.425443</td>\n      <td>-0.192835</td>\n      <td>-0.162892</td>\n      <td>0.221980</td>\n      <td>0.106563</td>\n      <td>-0.120675</td>\n      <td>...</td>\n      <td>0.010257</td>\n      <td>0.006590</td>\n      <td>0.058038</td>\n      <td>0.016583</td>\n      <td>-0.005697</td>\n      <td>-0.013547</td>\n      <td>-0.005410</td>\n      <td>-0.015903</td>\n      <td>0.006741</td>\n      <td>43.768915</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 205 columns</p>\n</div>"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the first three observations of the web data\n",
    "web_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(14227, 205)"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect web data shape\n",
    "web_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "              F1                                       Company_name NACE_code  \\\n375871  101613.0         ZZF ZWEIRADZENTRUM FERNWALD-STEINBACH GMBH      4540   \n375872  371556.0  WIRTSCHAFTSGEMEINSCHAFT ZOOLOGISCHER FACHBETRI...      9499   \n375873  275499.0  HTVG-GES. FÜR TECHNOLOGIEENTWICKLUNG U. VERMÖG...      7490   \n375874   96747.0      ZAHNRAD- UND ZERSPANUNGSTECHNIK STELLJES GMBH      2815   \n375875  375875.0      ZZV ZEITUNGS- UND ZEITSCHRIFTEN VERTRIEB GMBH      5813   \n\n        Op__Rev_th_EUR_Last_avail__yr Web_site_addresses  \\\n375871                         1201.0    www.zzf-gmbh.de   \n375872                            NaN         www.zzf.de   \n375873                            NaN  www.zzh-herten.de   \n375874                         1919.0    www.zzt-gmbh.de   \n375875                            NaN    www.zzv-gmbh.de   \n\n        Cash_flow_th_EUR_Last_avail__yr  Number_of_employees_Last_avail__  \\\n375871                              NaN                               NaN   \n375872                              NaN                               NaN   \n375873                              NaN                               NaN   \n375874                              NaN                               NaN   \n375875                              NaN                               NaN   \n\n        Total_assets_th_EUR_Last_avail__  Long_term_debt_th_EUR_Last_avail  \\\n375871                          2603.584                          1324.195   \n375872                         11585.822                          1534.590   \n375873                         16624.000                          9182.000   \n375874                          8178.178                           262.672   \n375875                         -1240.258                          -863.000   \n\n        Loans_th_EUR_Last_avail__yr  Capital_th_EUR_Last_avail__yr  \\\n375871                          NaN                        749.948   \n375872                          NaN                        206.000   \n375873                      -4538.0                      -2786.000   \n375874                       9408.0                        889.565   \n375875                      -9949.0                       -428.435   \n\n        Sales_th_EUR_Last_avail__yr  Gross_profit_th_EUR_Last_avail__  \\\n375871                       1856.0                               NaN   \n375872                          NaN                               NaN   \n375873                          NaN                               NaN   \n375874                       1764.0                               NaN   \n375875                          NaN                               NaN   \n\n        Profit_margin___Last_avail__yr  Liquidity_ratio_x_Last_avail__yr  \\\n375871                             NaN                               NaN   \n375872                             NaN                          6.354512   \n375873                             NaN                         -0.469264   \n375874                             NaN                          3.221941   \n375875                             NaN                          4.142150   \n\n        Average_cost_of_employee__th__EU  Profit_per_employee__th__EUR_Las  \\\n375871                               NaN                               NaN   \n375872                               NaN                               NaN   \n375873                               NaN                               NaN   \n375874                               NaN                               NaN   \n375875                               NaN                               NaN   \n\n        Total_assets_per_employee__th__E  Earnings_yield_______current  \n375871                               NaN                           NaN  \n375872                               NaN                           NaN  \n375873                               NaN                           NaN  \n375874                               NaN                           NaN  \n375875                               NaN                           NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F1</th>\n      <th>Company_name</th>\n      <th>NACE_code</th>\n      <th>Op__Rev_th_EUR_Last_avail__yr</th>\n      <th>Web_site_addresses</th>\n      <th>Cash_flow_th_EUR_Last_avail__yr</th>\n      <th>Number_of_employees_Last_avail__</th>\n      <th>Total_assets_th_EUR_Last_avail__</th>\n      <th>Long_term_debt_th_EUR_Last_avail</th>\n      <th>Loans_th_EUR_Last_avail__yr</th>\n      <th>Capital_th_EUR_Last_avail__yr</th>\n      <th>Sales_th_EUR_Last_avail__yr</th>\n      <th>Gross_profit_th_EUR_Last_avail__</th>\n      <th>Profit_margin___Last_avail__yr</th>\n      <th>Liquidity_ratio_x_Last_avail__yr</th>\n      <th>Average_cost_of_employee__th__EU</th>\n      <th>Profit_per_employee__th__EUR_Las</th>\n      <th>Total_assets_per_employee__th__E</th>\n      <th>Earnings_yield_______current</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>375871</th>\n      <td>101613.0</td>\n      <td>ZZF ZWEIRADZENTRUM FERNWALD-STEINBACH GMBH</td>\n      <td>4540</td>\n      <td>1201.0</td>\n      <td>www.zzf-gmbh.de</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2603.584</td>\n      <td>1324.195</td>\n      <td>NaN</td>\n      <td>749.948</td>\n      <td>1856.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>375872</th>\n      <td>371556.0</td>\n      <td>WIRTSCHAFTSGEMEINSCHAFT ZOOLOGISCHER FACHBETRI...</td>\n      <td>9499</td>\n      <td>NaN</td>\n      <td>www.zzf.de</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11585.822</td>\n      <td>1534.590</td>\n      <td>NaN</td>\n      <td>206.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.354512</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>375873</th>\n      <td>275499.0</td>\n      <td>HTVG-GES. FÜR TECHNOLOGIEENTWICKLUNG U. VERMÖG...</td>\n      <td>7490</td>\n      <td>NaN</td>\n      <td>www.zzh-herten.de</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16624.000</td>\n      <td>9182.000</td>\n      <td>-4538.0</td>\n      <td>-2786.000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.469264</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>375874</th>\n      <td>96747.0</td>\n      <td>ZAHNRAD- UND ZERSPANUNGSTECHNIK STELLJES GMBH</td>\n      <td>2815</td>\n      <td>1919.0</td>\n      <td>www.zzt-gmbh.de</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8178.178</td>\n      <td>262.672</td>\n      <td>9408.0</td>\n      <td>889.565</td>\n      <td>1764.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.221941</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>375875</th>\n      <td>375875.0</td>\n      <td>ZZV ZEITUNGS- UND ZEITSCHRIFTEN VERTRIEB GMBH</td>\n      <td>5813</td>\n      <td>NaN</td>\n      <td>www.zzv-gmbh.de</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-1240.258</td>\n      <td>-863.000</td>\n      <td>-9949.0</td>\n      <td>-428.435</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.142150</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the first three observations of the commercial data\n",
    "commercial_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(375876, 19)"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect commercial data shape\n",
    "commercial_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Merge Commercial and Web data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we are going to merge the commercial and web datasets together to obtain a large dataset with **all possible explanatory variables**. Since the website address can be found in both datasets, we are going to **merge the commercial data with the web data by the website address**.\n",
    "\n",
    "In the web data, the website address can be found in the `NAME` variable, but it is not yet in the correct format.\n",
    "The website address is always located between the substring \"3A--\" and its domain extension \".at\" or \".de\" (the companies from which the websites were scraped were all located in Germany or Austria). \n",
    "\n",
    "So for example the website address for the observation with `NAME` variable \"n2-1327http%3A--www.auto-boehler.de-.txt.htm\" is www.auto-boehler.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**:\n",
    "    \n",
    "1) **Remove** every observation from the web data where the domain extention .at or .de cannot be found in the NAME variable (we are only interested in companies with a valid website address)\n",
    "       \n",
    "       \n",
    "2) Extract all the website adresses from ``NAME`` variable and store it in a **new column** called ``Web_site_addresses``\n",
    "    \n",
    "    \n",
    "3) **Add** \"www.\" to websites where this is missing (this is necessary to obtain the correct matches between the  web data and commercial data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0          n2-1313http%3A--www.melisard.de-.txt.htm\n1      n2-1327http%3A--www.auto-boehler.de-.txt.htm\n2    n2-1346http%3A--www.ac-metallteile.de-.txt.htm\n3      n2-1361http%3A--www.awelldigital.de-.txt.htm\n4         n2-1393http%3A--www.paulbooch.de-.txt.htm\nName: NAME, dtype: object"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_data_permute = web_data\n",
    "web_data_permute[\"NAME\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "#1\n",
    "web_data_permute = web_data_permute[~web_data_permute['NAME'].isin(['.at','.de'])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 URI  \\\n0  file://C:\\sas\\small2\\n2-1313http%3A--www.melis...   \n1  file://C:\\sas\\small2\\n2-1327http%3A--www.auto-...   \n2  file://C:\\sas\\small2\\n2-1346http%3A--www.ac-me...   \n3  file://C:\\sas\\small2\\n2-1361http%3A--www.awell...   \n4  file://C:\\sas\\small2\\n2-1393http%3A--www.paulb...   \n\n                                             NAME  TARGET  _DOCUMENT_  \\\n0        n2-1313http%3A--www.melisard.de-.txt.htm       0           1   \n1    n2-1327http%3A--www.auto-boehler.de-.txt.htm       0           2   \n2  n2-1346http%3A--www.ac-metallteile.de-.txt.htm       0           3   \n3    n2-1361http%3A--www.awelldigital.de-.txt.htm       0           4   \n4       n2-1393http%3A--www.paulbooch.de-.txt.htm       0           5   \n\n     _SVD_1    _SVD_2    _SVD_3    _SVD_4    _SVD_5    _SVD_6  ...  _SVD_193  \\\n0  0.390678 -0.118327  0.199925 -0.200753  0.053916 -0.104150  ...  0.011185   \n1  0.463090 -0.121661 -0.149195  0.222979 -0.097737  0.000127  ...  0.028640   \n2  0.425443 -0.192835 -0.162892  0.221980  0.106563 -0.120675  ...  0.006590   \n3  0.616054 -0.252506 -0.229162  0.217503  0.214909  0.093692  ... -0.006780   \n4  0.387577 -0.200605 -0.154955  0.029873 -0.036174  0.028454  ... -0.033938   \n\n   _SVD_194  _SVD_195  _SVD_196  _SVD_197  _SVD_198  _SVD_199  _SVD_200  \\\n0 -0.002033  0.005218 -0.007452  0.007244 -0.005440  0.012882 -0.000899   \n1  0.020099 -0.013749 -0.006084 -0.004447  0.032138  0.020648 -0.009978   \n2  0.058038  0.016583 -0.005697 -0.013547 -0.005410 -0.015903  0.006741   \n3  0.034025  0.026627  0.019547 -0.012901 -0.004580  0.031348  0.020085   \n4  0.025743  0.021395  0.039515  0.104573 -0.029109 -0.028996 -0.010934   \n\n    _SVDLEN_     Web_site_addresses  \n0  62.478443        www.melisard.de  \n1  36.631883    www.auto-boehler.de  \n2  43.768915  www.ac-metallteile.de  \n3  30.409164    www.awelldigital.de  \n4  28.295083       www.paulbooch.de  \n\n[5 rows x 206 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>URI</th>\n      <th>NAME</th>\n      <th>TARGET</th>\n      <th>_DOCUMENT_</th>\n      <th>_SVD_1</th>\n      <th>_SVD_2</th>\n      <th>_SVD_3</th>\n      <th>_SVD_4</th>\n      <th>_SVD_5</th>\n      <th>_SVD_6</th>\n      <th>...</th>\n      <th>_SVD_193</th>\n      <th>_SVD_194</th>\n      <th>_SVD_195</th>\n      <th>_SVD_196</th>\n      <th>_SVD_197</th>\n      <th>_SVD_198</th>\n      <th>_SVD_199</th>\n      <th>_SVD_200</th>\n      <th>_SVDLEN_</th>\n      <th>Web_site_addresses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>file://C:\\sas\\small2\\n2-1313http%3A--www.melis...</td>\n      <td>n2-1313http%3A--www.melisard.de-.txt.htm</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.390678</td>\n      <td>-0.118327</td>\n      <td>0.199925</td>\n      <td>-0.200753</td>\n      <td>0.053916</td>\n      <td>-0.104150</td>\n      <td>...</td>\n      <td>0.011185</td>\n      <td>-0.002033</td>\n      <td>0.005218</td>\n      <td>-0.007452</td>\n      <td>0.007244</td>\n      <td>-0.005440</td>\n      <td>0.012882</td>\n      <td>-0.000899</td>\n      <td>62.478443</td>\n      <td>www.melisard.de</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>file://C:\\sas\\small2\\n2-1327http%3A--www.auto-...</td>\n      <td>n2-1327http%3A--www.auto-boehler.de-.txt.htm</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.463090</td>\n      <td>-0.121661</td>\n      <td>-0.149195</td>\n      <td>0.222979</td>\n      <td>-0.097737</td>\n      <td>0.000127</td>\n      <td>...</td>\n      <td>0.028640</td>\n      <td>0.020099</td>\n      <td>-0.013749</td>\n      <td>-0.006084</td>\n      <td>-0.004447</td>\n      <td>0.032138</td>\n      <td>0.020648</td>\n      <td>-0.009978</td>\n      <td>36.631883</td>\n      <td>www.auto-boehler.de</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>file://C:\\sas\\small2\\n2-1346http%3A--www.ac-me...</td>\n      <td>n2-1346http%3A--www.ac-metallteile.de-.txt.htm</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.425443</td>\n      <td>-0.192835</td>\n      <td>-0.162892</td>\n      <td>0.221980</td>\n      <td>0.106563</td>\n      <td>-0.120675</td>\n      <td>...</td>\n      <td>0.006590</td>\n      <td>0.058038</td>\n      <td>0.016583</td>\n      <td>-0.005697</td>\n      <td>-0.013547</td>\n      <td>-0.005410</td>\n      <td>-0.015903</td>\n      <td>0.006741</td>\n      <td>43.768915</td>\n      <td>www.ac-metallteile.de</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>file://C:\\sas\\small2\\n2-1361http%3A--www.awell...</td>\n      <td>n2-1361http%3A--www.awelldigital.de-.txt.htm</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0.616054</td>\n      <td>-0.252506</td>\n      <td>-0.229162</td>\n      <td>0.217503</td>\n      <td>0.214909</td>\n      <td>0.093692</td>\n      <td>...</td>\n      <td>-0.006780</td>\n      <td>0.034025</td>\n      <td>0.026627</td>\n      <td>0.019547</td>\n      <td>-0.012901</td>\n      <td>-0.004580</td>\n      <td>0.031348</td>\n      <td>0.020085</td>\n      <td>30.409164</td>\n      <td>www.awelldigital.de</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>file://C:\\sas\\small2\\n2-1393http%3A--www.paulb...</td>\n      <td>n2-1393http%3A--www.paulbooch.de-.txt.htm</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0.387577</td>\n      <td>-0.200605</td>\n      <td>-0.154955</td>\n      <td>0.029873</td>\n      <td>-0.036174</td>\n      <td>0.028454</td>\n      <td>...</td>\n      <td>-0.033938</td>\n      <td>0.025743</td>\n      <td>0.021395</td>\n      <td>0.039515</td>\n      <td>0.104573</td>\n      <td>-0.029109</td>\n      <td>-0.028996</td>\n      <td>-0.010934</td>\n      <td>28.295083</td>\n      <td>www.paulbooch.de</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 206 columns</p>\n</div>"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2\n",
    "web_data_permute[\"Web_site_addresses\"] = web_data_permute['NAME'].str.extract(r'3A--(.+?)-.txt.htm')\n",
    "web_data_permute.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "#3\n",
    "web_data_permute[\"Web_site_addresses\"] = np.where(web_data_permute[\"Web_site_addresses\"].str.contains(\"www.\"),web_data_permute[\"Web_site_addresses\"], 'www.' + web_data_permute[\"Web_site_addresses\"])\n",
    "\n",
    "#or\n",
    "#web_data[\"Web_site_addresses\"] = web_data[\"Web_site_addresses\"].apply(lambda x: \"www.\" + x if \"www.\" not in x else x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "web_data = web_data_permute\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether your code was correct by comparing the `NAME` variable with the newly created `Web_site_addresses` variable for the **first 5 observations** in the web data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                             NAME     Web_site_addresses\n0        n2-1313http%3A--www.melisard.de-.txt.htm        www.melisard.de\n1    n2-1327http%3A--www.auto-boehler.de-.txt.htm    www.auto-boehler.de\n2  n2-1346http%3A--www.ac-metallteile.de-.txt.htm  www.ac-metallteile.de\n3    n2-1361http%3A--www.awelldigital.de-.txt.htm    www.awelldigital.de\n4       n2-1393http%3A--www.paulbooch.de-.txt.htm       www.paulbooch.de",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NAME</th>\n      <th>Web_site_addresses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>n2-1313http%3A--www.melisard.de-.txt.htm</td>\n      <td>www.melisard.de</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>n2-1327http%3A--www.auto-boehler.de-.txt.htm</td>\n      <td>www.auto-boehler.de</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>n2-1346http%3A--www.ac-metallteile.de-.txt.htm</td>\n      <td>www.ac-metallteile.de</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>n2-1361http%3A--www.awelldigital.de-.txt.htm</td>\n      <td>www.awelldigital.de</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>n2-1393http%3A--www.paulbooch.de-.txt.htm</td>\n      <td>www.paulbooch.de</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "web_data[[\"NAME\", \"Web_site_addresses\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**:\n",
    "    \n",
    "1) **Merge** the web data and commercial data by the website address into a new DataFrame called basetable\n",
    "    \n",
    "    \n",
    "2) **Inspect** the first 3 observations of the basetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "        F1                                       Company_name NACE_code  \\\n0  35867.0                       1-2-3 GEBÄUDEMANAGEMENT GMBH      6832   \n1  78669.0                            1A PERSONALPARTNER GMBH      7820   \n2  49946.0  2 K KREATIVKONZEPT GESELLSCHAFT FÜR EFFEKTIVE ...      7311   \n3  55094.0                             3CAP TECHNOLOGIES GMBH      7112   \n4  78671.0                                  3D-SCHILLING GMBH      2896   \n\n   Op__Rev_th_EUR_Last_avail__yr         Web_site_addresses  \\\n0                         4807.0             www.1-2-3gm.de   \n1                         2133.0  www.1a-personalpartner.de   \n2                         2842.0                 www.2-k.de   \n3                         2696.0                www.3cap.de   \n4                         1992.0        www.3d-schilling.de   \n\n   Cash_flow_th_EUR_Last_avail__yr  Number_of_employees_Last_avail__  \\\n0                              NaN                               NaN   \n1                              NaN                               NaN   \n2                              NaN                               NaN   \n3                              NaN                               NaN   \n4                              NaN                               NaN   \n\n   Total_assets_th_EUR_Last_avail__  Long_term_debt_th_EUR_Last_avail  \\\n0                           723.714                          -733.529   \n1                          7987.706                           872.000   \n2                         -3959.834                          -571.000   \n3                         -4088.190                          -463.000   \n4                          4847.704                           360.953   \n\n   Loans_th_EUR_Last_avail__yr  ...  _SVD_192  _SVD_193  _SVD_194  _SVD_195  \\\n0                      -1475.0  ... -0.045802 -0.020668 -0.034812 -0.019691   \n1                       8132.0  ...  0.012936  0.005614  0.025265  0.020918   \n2                      -5418.0  ...  0.010125  0.006644  0.009632  0.006227   \n3                      -6055.0  ...  0.009281  0.003556 -0.012276  0.011094   \n4                       5933.0  ...  0.032550 -0.029513  0.017344 -0.006178   \n\n   _SVD_196  _SVD_197  _SVD_198  _SVD_199  _SVD_200   _SVDLEN_  \n0 -0.008253  0.038824 -0.023298  0.064798  0.011593  26.117231  \n1  0.027040  0.007349 -0.036119 -0.039724  0.012309  30.332482  \n2  0.022338  0.004339 -0.006715  0.019218 -0.029916  60.150309  \n3 -0.002663  0.006079  0.002901 -0.005705 -0.008120  69.091369  \n4  0.002543  0.023631  0.003898  0.004457  0.044888  56.614633  \n\n[5 rows x 224 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>F1</th>\n      <th>Company_name</th>\n      <th>NACE_code</th>\n      <th>Op__Rev_th_EUR_Last_avail__yr</th>\n      <th>Web_site_addresses</th>\n      <th>Cash_flow_th_EUR_Last_avail__yr</th>\n      <th>Number_of_employees_Last_avail__</th>\n      <th>Total_assets_th_EUR_Last_avail__</th>\n      <th>Long_term_debt_th_EUR_Last_avail</th>\n      <th>Loans_th_EUR_Last_avail__yr</th>\n      <th>...</th>\n      <th>_SVD_192</th>\n      <th>_SVD_193</th>\n      <th>_SVD_194</th>\n      <th>_SVD_195</th>\n      <th>_SVD_196</th>\n      <th>_SVD_197</th>\n      <th>_SVD_198</th>\n      <th>_SVD_199</th>\n      <th>_SVD_200</th>\n      <th>_SVDLEN_</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>35867.0</td>\n      <td>1-2-3 GEBÄUDEMANAGEMENT GMBH</td>\n      <td>6832</td>\n      <td>4807.0</td>\n      <td>www.1-2-3gm.de</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>723.714</td>\n      <td>-733.529</td>\n      <td>-1475.0</td>\n      <td>...</td>\n      <td>-0.045802</td>\n      <td>-0.020668</td>\n      <td>-0.034812</td>\n      <td>-0.019691</td>\n      <td>-0.008253</td>\n      <td>0.038824</td>\n      <td>-0.023298</td>\n      <td>0.064798</td>\n      <td>0.011593</td>\n      <td>26.117231</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>78669.0</td>\n      <td>1A PERSONALPARTNER GMBH</td>\n      <td>7820</td>\n      <td>2133.0</td>\n      <td>www.1a-personalpartner.de</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7987.706</td>\n      <td>872.000</td>\n      <td>8132.0</td>\n      <td>...</td>\n      <td>0.012936</td>\n      <td>0.005614</td>\n      <td>0.025265</td>\n      <td>0.020918</td>\n      <td>0.027040</td>\n      <td>0.007349</td>\n      <td>-0.036119</td>\n      <td>-0.039724</td>\n      <td>0.012309</td>\n      <td>30.332482</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>49946.0</td>\n      <td>2 K KREATIVKONZEPT GESELLSCHAFT FÜR EFFEKTIVE ...</td>\n      <td>7311</td>\n      <td>2842.0</td>\n      <td>www.2-k.de</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-3959.834</td>\n      <td>-571.000</td>\n      <td>-5418.0</td>\n      <td>...</td>\n      <td>0.010125</td>\n      <td>0.006644</td>\n      <td>0.009632</td>\n      <td>0.006227</td>\n      <td>0.022338</td>\n      <td>0.004339</td>\n      <td>-0.006715</td>\n      <td>0.019218</td>\n      <td>-0.029916</td>\n      <td>60.150309</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>55094.0</td>\n      <td>3CAP TECHNOLOGIES GMBH</td>\n      <td>7112</td>\n      <td>2696.0</td>\n      <td>www.3cap.de</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-4088.190</td>\n      <td>-463.000</td>\n      <td>-6055.0</td>\n      <td>...</td>\n      <td>0.009281</td>\n      <td>0.003556</td>\n      <td>-0.012276</td>\n      <td>0.011094</td>\n      <td>-0.002663</td>\n      <td>0.006079</td>\n      <td>0.002901</td>\n      <td>-0.005705</td>\n      <td>-0.008120</td>\n      <td>69.091369</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>78671.0</td>\n      <td>3D-SCHILLING GMBH</td>\n      <td>2896</td>\n      <td>1992.0</td>\n      <td>www.3d-schilling.de</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4847.704</td>\n      <td>360.953</td>\n      <td>5933.0</td>\n      <td>...</td>\n      <td>0.032550</td>\n      <td>-0.029513</td>\n      <td>0.017344</td>\n      <td>-0.006178</td>\n      <td>0.002543</td>\n      <td>0.023631</td>\n      <td>0.003898</td>\n      <td>0.004457</td>\n      <td>0.044888</td>\n      <td>56.614633</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 224 columns</p>\n</div>"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "basetable = pd.merge(left=commercial_data, right= web_data, on=\"Web_site_addresses\", how= \"inner\")\n",
    "basetable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can inspect the **distribution of the dependent variable** which is represented by the `TARGET` variable. This variable indicates whether a company was successfully converted into a profitable customer (`TARGET`=1) or not (`TARGET`=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1    0.64343\n0    0.35657\nName: TARGET, dtype: float64"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the distribution of profitable vs non-profitable customers of the new basetable\n",
    "basetable[\"TARGET\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Variable Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NACE` code is a variable that came from the commercial data and indicates the **economic sector** of the company \n",
    "and could be an important predictor. Let's have a look in how many different economic sectors the companies from are dataset are active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different economic sectors: 526\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of different economic sectors: %s\" %len(set(basetable[\"NACE_code\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high number of different economic sectors present in our dataset could result in the model not finding a clear relationship between an economic sector and the probability of successful conversion.\n",
    "Hence, we are going to **extract the first digit** of the ``NACE_code``, which equals the general economic sector in which the company is active. \n",
    "\n",
    "For example, the economic sector *\"Manufacture of Electrical Household Appliances\"* with NACE code 2751 is a subcategory of the general economic sector *\"Manufacturing\"*, which is represented by the number 2 in the NACE code. This will **reduce the number of different categories** significantly and will help the model in finding a relationship between the economic sector of a company and the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**:\n",
    "    \n",
    "1) Extract the **first digit** of the ``NACE_code`` variable and store it in a new column called ``NACE_1``. **Compare** the ``NACE_code`` and ``NACE_1`` variables for the first 3 observations to inspect whether your code was correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "F1                               float64\nCompany_name                      object\nNACE_code                         object\nOp__Rev_th_EUR_Last_avail__yr    float64\nWeb_site_addresses                object\n                                  ...   \n_SVD_197                         float64\n_SVD_198                         float64\n_SVD_199                         float64\n_SVD_200                         float64\n_SVDLEN_                         float64\nLength: 224, dtype: object"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basetable.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   NACE_1 NACE_code\n0       6      6832\n1       7      7820\n2       7      7311\n3       7      7112\n4       2      2896",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>NACE_1</th>\n      <th>NACE_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>6832</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>7820</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>7311</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>7112</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>2896</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basetable[\"NACE_1\"] = basetable[\"NACE_code\"].apply(lambda x: int(x[0]))\n",
    "basetable[[\"NACE_1\",\"NACE_code\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are going to **drop** all the variables we don't need for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the columns we are going to drop\n",
    "columns_to_drop = [\"F1\", \"Company_name\", \"Web_site_addresses\", \"URI\", \"NAME\",\"_SVDLEN_\", \"_DOCUMENT_\", \"NACE_code\"]\n",
    "# drop these colums from the basetable\n",
    "basetable = basetable.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train - Val - Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this specific case, we will **train several models** and **select the best performing model as our final model**.\n",
    "Therefore, we will split our data into **3 different sets**: the *training set*, *validation set* and *test set*.\n",
    "\n",
    "   - The ``training set`` will be used for training all the models \n",
    "   - The ``validation set`` will be used for evaluating all the models and selecting the best model\n",
    "   - The ``test set`` will be used for evaluating the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data randomly into training and test set (set seed to 33 to replicate same results)\n",
    "basetable_train, basetable_test = train_test_split(basetable, test_size=0.6, random_state=33)\n",
    "# split test set randomly into validation and test set\n",
    "basetable_val, basetable_test = train_test_split(basetable_test, test_size=0.5, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3293, 217)\n",
      "(2470, 217)\n",
      "(2471, 217)\n"
     ]
    }
   ],
   "source": [
    "# check shapes\n",
    "print(basetable_train.shape)\n",
    "print(basetable_val.shape)\n",
    "print(basetable_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Missing Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to **handle missing values**. In general, there exist **2 strategies** for dealing with missing values: \n",
    "\n",
    "1) Removing variables with missing values.\n",
    "2) Imputing the missing values of the variables. \n",
    "\n",
    "Missing values of ``numeric variables`` are often imputed by the **mean** of the observed values of that variable, while missing values of ``categorical variables`` are often imputed by the **mode** of the observed values of that variable. These statistics are **calculated on the the training set** and are used to **impute the missing values of the training, validation AND test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Op__Rev_th_EUR_Last_avail__yr  :  1468\n",
      "Cash_flow_th_EUR_Last_avail__yr  :  2534\n",
      "Number_of_employees_Last_avail__  :  2127\n",
      "Total_assets_th_EUR_Last_avail__  :  437\n",
      "Long_term_debt_th_EUR_Last_avail  :  454\n",
      "Loans_th_EUR_Last_avail__yr  :  1387\n",
      "Capital_th_EUR_Last_avail__yr  :  460\n",
      "Sales_th_EUR_Last_avail__yr  :  1665\n",
      "Gross_profit_th_EUR_Last_avail__  :  3274\n",
      "Profit_margin___Last_avail__yr  :  2765\n",
      "Liquidity_ratio_x_Last_avail__yr  :  1365\n",
      "Average_cost_of_employee__th__EU  :  2707\n",
      "Profit_per_employee__th__EUR_Las  :  2127\n",
      "Total_assets_per_employee__th__E  :  2521\n",
      "Earnings_yield_______current  :  3281\n"
     ]
    }
   ],
   "source": [
    "# inspect missing values per variable in training set\n",
    "for col in basetable_train.columns:\n",
    "    col_missings = basetable_train[col].isnull().sum()\n",
    "    if col_missings > 0:\n",
    "        print(col, \" : \", col_missings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this specific case, we are first going to **drop variables** with more than 50% of the observations missing in the training set. For variables with less than 50% of the observations missings in the training set, we are going to **impute** the missing values with their mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**:\n",
    "    \n",
    "1) Get the **percentage of missing values** per variable in training set.\n",
    "    \n",
    "    \n",
    "2) **Store** the names of the variables with more than 50% of the observations missing **in a list** and **drop** these from the basetable.\n",
    "       \n",
    "       \n",
    "3) **Drop** the variables from the train, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_variables(dataframe):\n",
    "    total_number_observations = len(dataframe.index)\n",
    "    for col in dataframe.columns:\n",
    "        col_missings = dataframe[col].isnull().sum()\n",
    "        percentage_missing = col_missings/total_number_observations\n",
    "        if percentage_missing > 0.5:\n",
    "            del dataframe[col]\n",
    "            del basetable_test[col]\n",
    "            del basetable_val[col]\n",
    "\n",
    "for x in [basetable_train]:\n",
    "    drop_variables(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**:\n",
    "    \n",
    "1) Get the **mean** of all the numeric variables of the training set.\n",
    "    \n",
    "    \n",
    "2) **Impute** the missing values of the training, validation and test set.\n",
    "    \n",
    "    \n",
    "3) Inspect number of **missing values** in training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Op__Rev_th_EUR_Last_avail__yr',\n 'Total_assets_th_EUR_Last_avail__',\n 'Long_term_debt_th_EUR_Last_avail',\n 'Loans_th_EUR_Last_avail__yr',\n 'Capital_th_EUR_Last_avail__yr',\n 'Liquidity_ratio_x_Last_avail__yr',\n 'TARGET',\n '_SVD_1',\n '_SVD_2',\n '_SVD_3',\n '_SVD_4',\n '_SVD_5',\n '_SVD_6',\n '_SVD_7',\n '_SVD_8',\n '_SVD_9',\n '_SVD_10',\n '_SVD_11',\n '_SVD_12',\n '_SVD_13',\n '_SVD_14',\n '_SVD_15',\n '_SVD_16',\n '_SVD_17',\n '_SVD_18',\n '_SVD_19',\n '_SVD_20',\n '_SVD_21',\n '_SVD_22',\n '_SVD_23',\n '_SVD_24',\n '_SVD_25',\n '_SVD_26',\n '_SVD_27',\n '_SVD_28',\n '_SVD_29',\n '_SVD_30',\n '_SVD_31',\n '_SVD_32',\n '_SVD_33',\n '_SVD_34',\n '_SVD_35',\n '_SVD_36',\n '_SVD_37',\n '_SVD_38',\n '_SVD_39',\n '_SVD_40',\n '_SVD_41',\n '_SVD_42',\n '_SVD_43',\n '_SVD_44',\n '_SVD_45',\n '_SVD_46',\n '_SVD_47',\n '_SVD_48',\n '_SVD_49',\n '_SVD_50',\n '_SVD_51',\n '_SVD_52',\n '_SVD_53',\n '_SVD_54',\n '_SVD_55',\n '_SVD_56',\n '_SVD_57',\n '_SVD_58',\n '_SVD_59',\n '_SVD_60',\n '_SVD_61',\n '_SVD_62',\n '_SVD_63',\n '_SVD_64',\n '_SVD_65',\n '_SVD_66',\n '_SVD_67',\n '_SVD_68',\n '_SVD_69',\n '_SVD_70',\n '_SVD_71',\n '_SVD_72',\n '_SVD_73',\n '_SVD_74',\n '_SVD_75',\n '_SVD_76',\n '_SVD_77',\n '_SVD_78',\n '_SVD_79',\n '_SVD_80',\n '_SVD_81',\n '_SVD_82',\n '_SVD_83',\n '_SVD_84',\n '_SVD_85',\n '_SVD_86',\n '_SVD_87',\n '_SVD_88',\n '_SVD_89',\n '_SVD_90',\n '_SVD_91',\n '_SVD_92',\n '_SVD_93',\n '_SVD_94',\n '_SVD_95',\n '_SVD_96',\n '_SVD_97',\n '_SVD_98',\n '_SVD_99',\n '_SVD_100',\n '_SVD_101',\n '_SVD_102',\n '_SVD_103',\n '_SVD_104',\n '_SVD_105',\n '_SVD_106',\n '_SVD_107',\n '_SVD_108',\n '_SVD_109',\n '_SVD_110',\n '_SVD_111',\n '_SVD_112',\n '_SVD_113',\n '_SVD_114',\n '_SVD_115',\n '_SVD_116',\n '_SVD_117',\n '_SVD_118',\n '_SVD_119',\n '_SVD_120',\n '_SVD_121',\n '_SVD_122',\n '_SVD_123',\n '_SVD_124',\n '_SVD_125',\n '_SVD_126',\n '_SVD_127',\n '_SVD_128',\n '_SVD_129',\n '_SVD_130',\n '_SVD_131',\n '_SVD_132',\n '_SVD_133',\n '_SVD_134',\n '_SVD_135',\n '_SVD_136',\n '_SVD_137',\n '_SVD_138',\n '_SVD_139',\n '_SVD_140',\n '_SVD_141',\n '_SVD_142',\n '_SVD_143',\n '_SVD_144',\n '_SVD_145',\n '_SVD_146',\n '_SVD_147',\n '_SVD_148',\n '_SVD_149',\n '_SVD_150',\n '_SVD_151',\n '_SVD_152',\n '_SVD_153',\n '_SVD_154',\n '_SVD_155',\n '_SVD_156',\n '_SVD_157',\n '_SVD_158',\n '_SVD_159',\n '_SVD_160',\n '_SVD_161',\n '_SVD_162',\n '_SVD_163',\n '_SVD_164',\n '_SVD_165',\n '_SVD_166',\n '_SVD_167',\n '_SVD_168',\n '_SVD_169',\n '_SVD_170',\n '_SVD_171',\n '_SVD_172',\n '_SVD_173',\n '_SVD_174',\n '_SVD_175',\n '_SVD_176',\n '_SVD_177',\n '_SVD_178',\n '_SVD_179',\n '_SVD_180',\n '_SVD_181',\n '_SVD_182',\n '_SVD_183',\n '_SVD_184',\n '_SVD_185',\n '_SVD_186',\n '_SVD_187',\n '_SVD_188',\n '_SVD_189',\n '_SVD_190',\n '_SVD_191',\n '_SVD_192',\n '_SVD_193',\n '_SVD_194',\n '_SVD_195',\n '_SVD_196',\n '_SVD_197',\n '_SVD_198',\n '_SVD_199',\n '_SVD_200',\n 'NACE_1']"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get names numeric columns\n",
    "numeric_columns = basetable_train.select_dtypes(include=np.number).columns.tolist()\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "numeric_mean = pd.DataFrame(columns=[\"variable\", \"mean\"])\n",
    "count = 0\n",
    "for numeric_column in numeric_columns:\n",
    "\n",
    "    numeric_mean.loc[str(count)] = [numeric_column, basetable_train[numeric_column].mean()]\n",
    "    count += 1\n",
    "numeric_mean\n",
    "\"\"\"\n",
    "mean_dict = dict(basetable_train.mean())\n",
    "\n",
    "basetable_train = basetable_train.fillna(mean_dict)\n",
    "basetable_val = basetable_val.fillna(mean_dict)\n",
    "basetable_test = basetable_test.fillna(mean_dict)\n",
    "\n",
    "print(basetable_train.isnull().sum().sum())\n",
    "print(basetable_val.isnull().sum().sum())\n",
    "print(basetable_test.isnull().sum().sum())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to **standardize the numeric features**. The statistics required for standardizing the features are first extracted from the ``training set``. Afterwards, the features from the training, validation AND test set are standardized by using these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the numeric features\n",
    "all_columns = basetable_train.columns\n",
    "numeric_features = [obs for obs in all_columns if obs not in [\"TARGET\", \"NACE_1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import min max scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit scaler on all the numeric variables from training set\n",
    "scaler.fit(basetable_train[numeric_features])\n",
    "\n",
    "# scale features\n",
    "basetable_train[numeric_features] = scaler.transform(basetable_train[numeric_features])\n",
    "basetable_val[numeric_features] = scaler.transform(basetable_val[numeric_features])\n",
    "basetable_test[numeric_features] = scaler.transform(basetable_test[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       Op__Rev_th_EUR_Last_avail__yr  Total_assets_th_EUR_Last_avail__  \\\ncount                    3293.000000                       3293.000000   \nmean                        0.001911                          0.000947   \nstd                         0.020596                          0.019440   \nmin                         0.000000                          0.000000   \n25%                         0.000077                          0.000082   \n50%                         0.001177                          0.000150   \n75%                         0.001911                          0.000244   \nmax                         1.000000                          1.000000   \n\n       Long_term_debt_th_EUR_Last_avail  Loans_th_EUR_Last_avail__yr  \\\ncount                       3293.000000                  3293.000000   \nmean                           0.000677                     0.001609   \nstd                            0.017998                     0.017656   \nmin                            0.000000                     0.000000   \n25%                            0.000027                     0.000788   \n50%                            0.000047                     0.001609   \n75%                            0.000105                     0.001609   \nmax                            1.000000                     1.000000   \n\n       Capital_th_EUR_Last_avail__yr  Liquidity_ratio_x_Last_avail__yr  \\\ncount                    3293.000000                       3293.000000   \nmean                        0.025302                          0.042203   \nstd                         0.022804                          0.057795   \nmin                         0.000000                          0.000000   \n25%                         0.023246                          0.020880   \n50%                         0.023752                          0.042203   \n75%                         0.024357                          0.042203   \nmax                         1.000000                          1.000000   \n\n            TARGET       _SVD_1       _SVD_2       _SVD_3  ...     _SVD_192  \\\ncount  3293.000000  3293.000000  3293.000000  3293.000000  ...  3293.000000   \nmean      0.638627     0.595768     0.216884     0.166162  ...     0.423436   \nstd       0.480471     0.166911     0.211721     0.162343  ...     0.067853   \nmin       0.000000     0.000000     0.000000     0.000000  ...     0.000000   \n25%       0.000000     0.502885     0.095755     0.071363  ...     0.386163   \n50%       1.000000     0.624307     0.138669     0.117178  ...     0.423389   \n75%       1.000000     0.713176     0.218617     0.201285  ...     0.459552   \nmax       1.000000     1.000000     1.000000     1.000000  ...     1.000000   \n\n          _SVD_193     _SVD_194     _SVD_195     _SVD_196     _SVD_197  \\\ncount  3293.000000  3293.000000  3293.000000  3293.000000  3293.000000   \nmean      0.356348     0.396669     0.428151     0.491102     0.501764   \nstd       0.069343     0.062097     0.098082     0.068814     0.098024   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.318323     0.364895     0.372040     0.454920     0.450688   \n50%       0.357531     0.396761     0.426903     0.491756     0.503089   \n75%       0.394064     0.431065     0.480279     0.526504     0.555955   \nmax       1.000000     1.000000     1.000000     1.000000     1.000000   \n\n          _SVD_198     _SVD_199     _SVD_200       NACE_1  \ncount  3293.000000  3293.000000  3293.000000  3293.000000  \nmean      0.638130     0.507029     0.583741     4.739144  \nstd       0.075021     0.101492     0.073703     2.176934  \nmin       0.000000     0.000000     0.000000     0.000000  \n25%       0.597383     0.449773     0.542004     3.000000  \n50%       0.637795     0.505825     0.585950     4.000000  \n75%       0.678895     0.562664     0.624408     7.000000  \nmax       1.000000     1.000000     1.000000     9.000000  \n\n[8 rows x 208 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Op__Rev_th_EUR_Last_avail__yr</th>\n      <th>Total_assets_th_EUR_Last_avail__</th>\n      <th>Long_term_debt_th_EUR_Last_avail</th>\n      <th>Loans_th_EUR_Last_avail__yr</th>\n      <th>Capital_th_EUR_Last_avail__yr</th>\n      <th>Liquidity_ratio_x_Last_avail__yr</th>\n      <th>TARGET</th>\n      <th>_SVD_1</th>\n      <th>_SVD_2</th>\n      <th>_SVD_3</th>\n      <th>...</th>\n      <th>_SVD_192</th>\n      <th>_SVD_193</th>\n      <th>_SVD_194</th>\n      <th>_SVD_195</th>\n      <th>_SVD_196</th>\n      <th>_SVD_197</th>\n      <th>_SVD_198</th>\n      <th>_SVD_199</th>\n      <th>_SVD_200</th>\n      <th>NACE_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>...</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n      <td>3293.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.001911</td>\n      <td>0.000947</td>\n      <td>0.000677</td>\n      <td>0.001609</td>\n      <td>0.025302</td>\n      <td>0.042203</td>\n      <td>0.638627</td>\n      <td>0.595768</td>\n      <td>0.216884</td>\n      <td>0.166162</td>\n      <td>...</td>\n      <td>0.423436</td>\n      <td>0.356348</td>\n      <td>0.396669</td>\n      <td>0.428151</td>\n      <td>0.491102</td>\n      <td>0.501764</td>\n      <td>0.638130</td>\n      <td>0.507029</td>\n      <td>0.583741</td>\n      <td>4.739144</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.020596</td>\n      <td>0.019440</td>\n      <td>0.017998</td>\n      <td>0.017656</td>\n      <td>0.022804</td>\n      <td>0.057795</td>\n      <td>0.480471</td>\n      <td>0.166911</td>\n      <td>0.211721</td>\n      <td>0.162343</td>\n      <td>...</td>\n      <td>0.067853</td>\n      <td>0.069343</td>\n      <td>0.062097</td>\n      <td>0.098082</td>\n      <td>0.068814</td>\n      <td>0.098024</td>\n      <td>0.075021</td>\n      <td>0.101492</td>\n      <td>0.073703</td>\n      <td>2.176934</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000077</td>\n      <td>0.000082</td>\n      <td>0.000027</td>\n      <td>0.000788</td>\n      <td>0.023246</td>\n      <td>0.020880</td>\n      <td>0.000000</td>\n      <td>0.502885</td>\n      <td>0.095755</td>\n      <td>0.071363</td>\n      <td>...</td>\n      <td>0.386163</td>\n      <td>0.318323</td>\n      <td>0.364895</td>\n      <td>0.372040</td>\n      <td>0.454920</td>\n      <td>0.450688</td>\n      <td>0.597383</td>\n      <td>0.449773</td>\n      <td>0.542004</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.001177</td>\n      <td>0.000150</td>\n      <td>0.000047</td>\n      <td>0.001609</td>\n      <td>0.023752</td>\n      <td>0.042203</td>\n      <td>1.000000</td>\n      <td>0.624307</td>\n      <td>0.138669</td>\n      <td>0.117178</td>\n      <td>...</td>\n      <td>0.423389</td>\n      <td>0.357531</td>\n      <td>0.396761</td>\n      <td>0.426903</td>\n      <td>0.491756</td>\n      <td>0.503089</td>\n      <td>0.637795</td>\n      <td>0.505825</td>\n      <td>0.585950</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.001911</td>\n      <td>0.000244</td>\n      <td>0.000105</td>\n      <td>0.001609</td>\n      <td>0.024357</td>\n      <td>0.042203</td>\n      <td>1.000000</td>\n      <td>0.713176</td>\n      <td>0.218617</td>\n      <td>0.201285</td>\n      <td>...</td>\n      <td>0.459552</td>\n      <td>0.394064</td>\n      <td>0.431065</td>\n      <td>0.480279</td>\n      <td>0.526504</td>\n      <td>0.555955</td>\n      <td>0.678895</td>\n      <td>0.562664</td>\n      <td>0.624408</td>\n      <td>7.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>9.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 208 columns</p>\n</div>"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "basetable_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done all the data preprocessing, we can **start building our models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of fitting one logistic regression model on the training set, \n",
    "we are going to **fit multiple models** on the training set: \n",
    "        \n",
    "        - One model which is trained only with variables coming from the commercial data\n",
    "        - One model which is trained only with variables coming from the web data\n",
    "        - One model which is trained on the variables coming from the commercial and web data\n",
    "    \n",
    "This will allow us to compare the **predictive performance** of each model and to **investigate** whether the augmentation of the commercial data with the scraped web data increases the predictive performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**:\n",
    "    \n",
    "1) Create **a function** that fits a logistic regression model on a user-specified training set with a user-specified list of independent features and a certain dependent variable (**1**). Once the model is fit, the function should also make predictions on the user-specified evaluation set (**2**). These predictions should be the predicted probabilities instead of the predicted categories and can be obtained by the ``predict_proba`` function of a trained sklearn model.\n",
    "       \n",
    "    - The function should thus accept **4 parameters**: \n",
    "        - A list of the names of the features\n",
    "        - The name of the dependent variable\n",
    "        - The training dataset\n",
    "        - The evaluation dataset\n",
    "    - The function should **return** the predicted probabilities for each observation from the evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the function\n",
    "def fit_lr_with_features(features, dependent_variable, train_data, test_data):\n",
    "    # initialize logistic regression model\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    # fit logistic regression model on training set\n",
    "    lr_model.fit(X=train_data[features], y=train_data[dependent_variable])\n",
    "\n",
    "    # make predictions on test set\n",
    "    test_preds = lr_model.predict_proba(X=test_data[features])\n",
    "\n",
    "    # return predictions\n",
    "    return(test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we **define the features** coming from the web data and the commercial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features coming from web data\n",
    "web_data_features = [\"_SVD_%s\" %i for i in range(1, 200)]\n",
    "\n",
    "# define features coming from commercial data\n",
    "com_data_features = [\"Op__Rev_th_EUR_Last_avail__yr\",\n",
    "                      \"Total_assets_th_EUR_Last_avail__\", \n",
    "                      \"Long_term_debt_th_EUR_Last_avail\",\n",
    "                      \"Loans_th_EUR_Last_avail__yr\",\n",
    "                      \"Capital_th_EUR_Last_avail__yr\",\n",
    "                      \"Liquidity_ratio_x_Last_avail__yr\",\n",
    "                      \"NACE_1\"]\n",
    "\n",
    "# define features coming from web and commercial data\n",
    "all_data_features = web_data_features + com_data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we defined the features coming from the different datasets, we can use our function that we created in the previous exercise to train and evaluate a logistic regression model **for each separate set of features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**:\n",
    "    \n",
    "1) Get the predicted probabilities for the observations from the validation set of the model which was trained with the independent variables coming from the **web data** and the dependent variable being ``TARGET``\n",
    "       \n",
    "2) Get the predicted probabilities for the observations from the validation set of the model which was trained with the independent variables coming from the **commercial data** and the dependent variable being ``TARGET``\n",
    "    \n",
    "3) Get the predicted probabilities for the observations from the validation set of the model which was trained with the independent variables coming from the **web and commercial data** and the dependent variable being ``TARGET``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Boolean array expected for the condition, not object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [158]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m web_val_preds \u001B[38;5;241m=\u001B[39m fit_lr_with_features(features\u001B[38;5;241m=\u001B[39mweb_data_features, dependent_variable\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTARGET\u001B[39m\u001B[38;5;124m\"\u001B[39m, train_data\u001B[38;5;241m=\u001B[39mbasetable_train, test_data\u001B[38;5;241m=\u001B[39mbasetable_val)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# get predictions for model trained on features coming from commercial data\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m com_val_preds \u001B[38;5;241m=\u001B[39m \u001B[43mfit_lr_with_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommercial_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdependent_variable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTARGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbasetable_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbasetable_val\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# get predictions for model trained on features coming from web and commercial data\u001B[39;00m\n\u001B[0;32m     10\u001B[0m all_val_preds \u001B[38;5;241m=\u001B[39m fit_lr_with_features(features\u001B[38;5;241m=\u001B[39mall_data_features, dependent_variable\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTARGET\u001B[39m\u001B[38;5;124m\"\u001B[39m, train_data\u001B[38;5;241m=\u001B[39mbasetable_train, test_data\u001B[38;5;241m=\u001B[39mbasetable_val)\n",
      "Input \u001B[1;32mIn [156]\u001B[0m, in \u001B[0;36mfit_lr_with_features\u001B[1;34m(features, dependent_variable, train_data, test_data)\u001B[0m\n\u001B[0;32m      4\u001B[0m lr_model \u001B[38;5;241m=\u001B[39m LogisticRegression(max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# fit logistic regression model on training set\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m lr_model\u001B[38;5;241m.\u001B[39mfit(X\u001B[38;5;241m=\u001B[39m\u001B[43mtrain_data\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m]\u001B[49m, y\u001B[38;5;241m=\u001B[39mtrain_data[dependent_variable])\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# make predictions on test set\u001B[39;00m\n\u001B[0;32m     10\u001B[0m test_preds \u001B[38;5;241m=\u001B[39m lr_model\u001B[38;5;241m.\u001B[39mpredict_proba(X\u001B[38;5;241m=\u001B[39mtest_data[features])\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3492\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3490\u001B[0m \u001B[38;5;66;03m# Do we have a (boolean) DataFrame?\u001B[39;00m\n\u001B[0;32m   3491\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, DataFrame):\n\u001B[1;32m-> 3492\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3494\u001B[0m \u001B[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001B[39;00m\n\u001B[0;32m   3495\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m com\u001B[38;5;241m.\u001B[39mis_bool_indexer(key):\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[0;32m    310\u001B[0m     )\n\u001B[1;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:10955\u001B[0m, in \u001B[0;36mDataFrame.where\u001B[1;34m(self, cond, other, inplace, axis, level, errors, try_cast)\u001B[0m\n\u001B[0;32m  10942\u001B[0m \u001B[38;5;129m@deprecate_nonkeyword_arguments\u001B[39m(\n\u001B[0;32m  10943\u001B[0m     version\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, allowed_args\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcond\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mother\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m  10944\u001B[0m )\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m  10953\u001B[0m     try_cast\u001B[38;5;241m=\u001B[39mlib\u001B[38;5;241m.\u001B[39mno_default,\n\u001B[0;32m  10954\u001B[0m ):\n\u001B[1;32m> 10955\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcond\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtry_cast\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:9308\u001B[0m, in \u001B[0;36mNDFrame.where\u001B[1;34m(self, cond, other, inplace, axis, level, errors, try_cast)\u001B[0m\n\u001B[0;32m   9300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m try_cast \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mno_default:\n\u001B[0;32m   9301\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   9302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtry_cast keyword is deprecated and will be removed in a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   9303\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfuture version.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   9304\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m   9305\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m   9306\u001B[0m     )\n\u001B[1;32m-> 9308\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_where\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcond\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:9069\u001B[0m, in \u001B[0;36mNDFrame._where\u001B[1;34m(self, cond, other, inplace, axis, level, errors)\u001B[0m\n\u001B[0;32m   9067\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m dt \u001B[38;5;129;01min\u001B[39;00m cond\u001B[38;5;241m.\u001B[39mdtypes:\n\u001B[0;32m   9068\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_bool_dtype(dt):\n\u001B[1;32m-> 9069\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg\u001B[38;5;241m.\u001B[39mformat(dtype\u001B[38;5;241m=\u001B[39mdt))\n\u001B[0;32m   9070\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   9071\u001B[0m     \u001B[38;5;66;03m# GH#21947 we have an empty DataFrame/Series, could be object-dtype\u001B[39;00m\n\u001B[0;32m   9072\u001B[0m     cond \u001B[38;5;241m=\u001B[39m cond\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mbool\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Boolean array expected for the condition, not object"
     ]
    }
   ],
   "source": [
    "# get predictions for model trained on features coming from web data\n",
    "web_val_preds = fit_lr_with_features(features=web_data_features, dependent_variable=\"TARGET\", train_data=basetable_train, test_data=basetable_val)\n",
    "\n",
    "\n",
    "# get predictions for model trained on features coming from commercial data\n",
    "com_val_preds = fit_lr_with_features(features=commercial_data, dependent_variable=\"TARGET\", train_data=basetable_train, test_data=basetable_val)\n",
    "\n",
    "\n",
    "# get predictions for model trained on features coming from web and commercial data\n",
    "all_val_preds = fit_lr_with_features(features=all_data_features, dependent_variable=\"TARGET\", train_data=basetable_train, test_data=basetable_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can **evaluate every model** by comparing its predicted probabilites with the true values.\n",
    "The ``AUC`` is a good evaluation metric for evaluating a binary model since it represents the probability of ranking a positive example higher than a negative example. Hence, an AUC of 1 represents a perfect model, while an AUC of 0.5 represents a random model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get auc score of the web model\n",
    "fpr_w, tpr_w, threshold = roc_curve(y_true=basetable_val[\"TARGET\"], y_score=web_val_preds[:, 1])\n",
    "auc_w = roc_auc_score(y_true=basetable_val[\"TARGET\"], y_score=web_val_preds[:, 1])\n",
    "\n",
    "# get auc score of the commercial model\n",
    "fpr_c, tpr_c, threshold = roc_curve(y_true=basetable_val[\"TARGET\"], y_score=com_val_preds[:, 1])\n",
    "auc_c = roc_auc_score(y_true=basetable_val[\"TARGET\"], y_score=com_val_preds[:, 1])\n",
    "\n",
    "# get auc score of the complete model\n",
    "fpr_a, tpr_a, threshold = roc_curve(y_true=basetable_val[\"TARGET\"], y_score=all_val_preds[:, 1])\n",
    "auc_a = roc_auc_score(y_true=basetable_val[\"TARGET\"], y_score=all_val_preds[:, 1])\n",
    "\n",
    "# create plot with roc curves of each model\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr_w, tpr_w, label=\"Web model -- auc: %s\" %round(auc_w, 2))\n",
    "plt.plot(fpr_c, tpr_c, label=\"Com model -- auc: %s\" %round(auc_c, 2))\n",
    "plt.plot(fpr_a, tpr_a, label=\"All model -- auc: %s\" %round(auc_a, 2))\n",
    "plt.plot([0, 1], [0, 1],'r--', label=\"Random model -- auc: 0.5\")\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models trained on the features coming from the **web data** (Web model) and on **all the features** (All model) have the highest AUC. However, the model trained on the web data (Web model) has less features and hence is **less complex**. Therefore, we will choose the model trained on the web data as our final model (i.e., Occam's razor). We will **evaluate this model** on the test set to get our final unbiased estimate of the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8**:\n",
    "    \n",
    "Get the **predicted probabilities** for all the observations in the test set by using the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train final model on training set and get predictions on test set\n",
    "final_model_test_preds = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9**:\n",
    "    \n",
    "Get the ``AUC`` of the **final model** on the test set and **plot** the ``ROC`` curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Lift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will evaluate our model in terms of the ``lift``. This measures how much the model performs better than a random model **for different chunks of data**. More specifically, the observations are **first sorted** by their predicted probability of being positive (in this case being successfully converted into a customer). Next, these observations will be **grouped into different chunks** (most often into percentiles with each percentile containing 1% of the observations). So the first chunk will contain observations with higher predicted probabilites than the second chunk and so on, meaning that the model is more certain for making positive predictions in the first chunk than the second chunk and so on. So we can expect that the number of correctly identified positives in the first chunk will be higher than the second chunk and so on.\n",
    "Finally, for each chunk, the model performance is **compared with the random model** by dividing the proportion of correctly identified positives with the general proportion of positives (which is the prediction of the random model). The result is a **lift score** for each chunk, indicating how much the model performs better than the random model for that particular chunk. \n",
    "\n",
    "For **example**: \n",
    "Suppose that there are 1000 companies in the test set and that the proportion of succesfully converted companies\n",
    "equals 0.6.\n",
    "First we will sort these companies by their predicted probability of being successfully converted into a customer.\n",
    "Next we will split these ranked companies into percentiles, such that each chunk will contain 10 companies.\n",
    "Now suppose that in the first chunk there are 9 companies of which the true label was 1 and 1 company of which the true label was 0. \n",
    "Then the lift score of this chunk is 0.9 / 0.6 = 1.5, meaning that the model is 1.5 times better than the random model for this chunk. \n",
    "Now suppose that in the second chunk there are only 7 companies of which the true label was 1 and 3 companies of which the true label was 0. \n",
    "Then the lift score of this chunk is 0.7 / 0.6 = 1.16, meaning that the model is 1.16 times better than the random model for this chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/lift.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10**:\n",
    "    \n",
    "1) **Extract the label** and the **predicted probability** of being successfully converted into a customer of each observation in the test set and **store** these 2 variables into a new DataFrame.\n",
    "        \n",
    "2) **Sort** the observations of this new DataFrame by the predicted probability in *descending* order.\n",
    "    \n",
    "3) Get the **proportion** of companies being successfully converted in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dependent variable and predictions of test set in new DataFrame \n",
    "label_pred = \n",
    "\n",
    "\n",
    "# sort DataFrame by predicted probability\n",
    "label_pred = \n",
    "\n",
    "# check\n",
    "label_pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get global proportion of profitable customers in test set\n",
    "prop_profitable_cust_test = \n",
    "\n",
    "# check \n",
    "print(\"Proportion of profitable customers: %s\" %prop_profitable_cust_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate max lift\n",
    "max_lift = 1. / prop_profitable_cust_test\n",
    "# check\n",
    "print(\"Max lift: %s\" %max_lift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11**:\n",
    "    \n",
    "Now that the companies are ranked according to their predicted probability of being successfully converted \n",
    "into a customer, we can calculated the lift score for each percentile. \n",
    "In the following code, companies are already properly assigned to each chunk by making use of a for loop.\n",
    "\n",
    "1) Complete the code by **calculating the lift score for each chunk** and store this lift score in the ``lift_scores`` list.\n",
    "        \n",
    "        \n",
    "2) **Plot the lift scores for each chunk** with the chunk number on the X-axis and the lift score on the Y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list for storing lift scores \n",
    "lift_scores = []\n",
    "\n",
    "# loop through percentiles\n",
    "for i in reversed(range(100)):\n",
    "    # divide dataframes into percentiles of predicted probabilities\n",
    "    start_perc = label_pred[\"prediction\"].quantile(i / 100.)\n",
    "    end_perc = label_pred[\"prediction\"].quantile((i+1) / 100.)\n",
    "    chunk = label_pred[(label_pred[\"prediction\"] >= start_perc) &  (label_pred[\"prediction\"] < end_perc)]\n",
    "    \n",
    "    # get lift score of each chunk\n",
    "    \n",
    "    \n",
    "    # add chunk to lift_scores list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot lift curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(100), lift_scores, label=\"Lift curve\")\n",
    "plt.plot(range(100), [max_lift for i in range(100)], label=\"Max lift\")\n",
    "plt.plot(range(100), [1 for i in range(100)], label=\"Random model\")\n",
    "plt.xlabel(\"Percentile\")\n",
    "plt.ylabel(\"Lift\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
